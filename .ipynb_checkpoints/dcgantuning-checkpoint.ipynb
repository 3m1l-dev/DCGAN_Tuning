{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "os.makedirs('images', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Enable GPU\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring dataset and checking images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Configure data loader and image parameters\n",
    "\n",
    "img_dim = 32\n",
    "img_channels = 3\n",
    "\n",
    "os.makedirs('./data/cifar10', exist_ok=True)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(img_dim),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data/cifar10', train=True, download=True,\n",
    "                       transform=transform)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.detach().cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAFfVJREFUeJzt3X+Q3WV1x/H3SUgIMYkBQmJIwBDkxyBiwB0GlSJFpUidAVsHpcUyIzXakWmZajsMdQrO0BltRYfWURskQ2ytiIJjVGYkIg6lYwkBQhISCCEukJhkCQETDCG/Tv+4Xzqb8D1n797d/d4Nz+c1k8nuc/b5fp/97p773XvPfZ7H3B0RKc+Ybg9ARLpDyS9SKCW/SKGU/CKFUvKLFErJL1IoJb9IoZT8IoVS8osU6rChdDazi4CbgbHAt939S9nXjzPzCUFsX3aeDsa2e0zc66jx8WPey7vikezsYByjRfaDPnbq+DC2f//+MLZr994wNmZc/TFf2rE77BNHZDDcva2UsU7f3mtmY4G1wAeBDcBDwOXuvjrqM9nM3xXEtiXnin41sz9beicfHsauOH5SGHvg8RfC2EPJ+Ua7GUnshj99axjbtTN+yHvi2efD2MRj649515Jnwj5xRAaj3eQfyp/9ZwPr3H29u+8GbgcuGcLxRKRBQ0n+WcBz/T7fULWJyCFgSM/522Fm84H5APEf4iLStKHc+TcCx/X7fHbVdgB3X+DuPe7eM24IJxOR4TWU5H8IOMnMTjCz8cDHgcXDMywRGWkd/9nv7nvN7Grg57RKfQvd/fGsz6vA+iAWlQABJgbtS7Px7Xg1Dj4ex+Li1aFtSxJbvy2+B+zc+lIYe2JtfMwzptb3a/r6vj1oD0tSQCnL2wzpOb+73w3cPUxjEZEG6R1+IoVS8osUSskvUiglv0ihlPwihRrxd/j1t4cD3w/cLU8ksUcbG8Xo8fX7fhPGzpoc99u+Jztq/WzAKUmP171DrE0nJLHz335kbfucdS+GfZYnVeJsjGOTWDZr9eigveeIuM/PX0kO2Cbd+UUKpeQXKZSSX6RQSn6RQin5RQrV6Kv9o0U8VaVM2QvH/7Mjjr07WRtswqRpte3nvzteE7Dv178PY/Hiaq1VZEJTp9c2f+YLV4RdbvvGv4WxuzfFp+r0BfjoexszDK/oZ3TnFymUkl+kUEp+kUIp+UUKpeQXKZSSX6RQh3Spb2YSSyoy1Bd/ZLCWJwsD/vV5F9e2r1uxPOzzn5+8MIxd/Znrw9jTyayZ1dvrVw288Yr5YZ97lv4ijL3ykzXxyTo0i/rZU9tI6qzDQHd+kUIp+UUKpeQXKZSSX6RQSn6RQin5RQo1pFKfmfUCO2gtUbbX3Xs6PdYpSeyCoP34pM+/JrH6+Wbl+tiH3hHGVm+N+618aGUYW7z4l7Xtu7clB5w+Nwz1ZovgJe5b+XRte9/e8WGfvl0jUQE/LozMOaU+bVavjfdD+9jb6gvW9zy7rO0RDcd3+YfunvxERWQ00p/9IoUaavI7cI+ZPWxm8VumRGTUGeqf/ee6+0Yzmw4sMbMn3P3+/l9QPSjogUFklBnSnd/dN1b/9wE/As6u+ZoF7t4zlBcDRWT4dZz8ZvYmM5v82sfAhcCq4RqYiIwsc/fOOprNpXW3h9bTh/9y938aoE94sg8m/Y4N2rclfT6QxLJ+X0xikfclsY8mUw+3JlMPsy3FstLK7qB9QtLn0mRLrlXJxLLNyTGjZTqz55n1xcGWeHOtztz5sx+Fsenb49+QFb9cGsY+e8u/D2lMg/F3x9S3f+dF2LzHrZ1jdPyc393XA+/stL+IdJdKfSKFUvKLFErJL1IoJb9IoZT8IoUaNQt4vu7dQf3MDtpvS/qsT2LnJbGs1DcxaD856TMlKedFZTmA8zsYB8Q/0Oxc25NyXjZzsn5pzIHPF4l+zgBfT2KdTPjbsPz+MHb2n3w4jB07fWoYOykp9T3V3rDa9ovn69u3D+IYuvOLFErJL1IoJb9IoZT8IoVS8osUqtFX+ycD5wSxbAutaLW1+DVZ+G1bI3q9rOoQrf0XrwaXP7q+nMQG86ptf1ElIJvYk1UPdiaxaPIOwK6gPasCnJHEbkxi2XWMzjf1V78K+2ydlBxwdjTNDC77o3h62rqt8ShX7ay/Wn3rN4R9Ljr1LbXtm9auC/scTHd+kUIp+UUKpeQXKZSSX6RQSn6RQin5RQrVaKnvVSAqRGSlrWhFtWwtu6OS2EtJLCtFReWr7CJOSWLZI290LsjHGJXfsrJcFstKhHHRK77GWVku3qwrLwVn1ypaZ3DrkkfDPo8ksdnvemsYW7v8mTD28rR4McddwS/41j3B7B1gw7P1V3L37uxqHEh3fpFCKflFCqXkFymUkl+kUEp+kUIp+UUKNeB2XWa2kNYEuj53P71qOwr4PjAH6AUuc/cBd1TKtuv6UNIvKl5kk6/O6uB4APOSWFTa6qQ8OJBoBiHkpcqoLJqVHLNZidn31kkZMyuLZmsCjnnHuDg4qX6GG8DuVfVXpG/H78M+vck44hX8YHUSy+ba/SyJRd4ftC8Ftnt723W1c+e/DbjooLZrgXvd/STg3upzETmEDJj87n4/r7+hXAIsqj5eBFw6zOMSkRHW6XP+Ge7+2qLUm4EZwzQeEWnIkN/e6+6ePZc3s/nA/KGeR0SGV6d3/i1mNhOg+r8v+kJ3X+DuPe7e0+G5RGQEdJr8i4Erq4+vBH48PMMRkaYM+Ge/mX2P1u5R08xsA3A98CXgDjO7CngGuGyoA1mexKJFJE9N+mQz1bISVRaLZrhlM9WiWWWQl9EmvjkZR/KQPT0ouGYLcWaxrKx47PviGW5be4MZbvHEt9zKPWFoO8+FsYmn1M+mm3fGe8I+U36wJIxlJbs5Sez4sXHsgWC/sd8lx4sWXR3M3XzA5Hf3y4NQVGoUkUOA3uEnUiglv0ihlPwihVLyixRKyS9SqEYX8BxLPCsqG0i0GGenM87WJ7GvJLHoXUrZxIZOFiYFeDmp82T9okfzrGSXLYR6xSlHhrG+w+LZdNueqa/pZTMxsxmQWTk1K1VOfHJTbfvWoB1g4uT4eHOTlUR7n45jc06NZyX+5Uv18xlv2hjPuJ1yzBG17WNf1AKeIjIAJb9IoZT8IoVS8osUSskvUiglv0ihGi31TSReIPO3Sb9sMcvsXJFHkli2Cmk01+uMpE829mzmYTZ7LIvdl8QiJyWxc+ceH8ZefmRFGIuKgJ3ebbLS7YYkFpWDs2u/fkccm57Eet4e1whfejzueFowg/OP41PxngvOqW3/73uWJb0OpDu/SKGU/CKFUvKLFErJL1IoJb9IoQbcrms4TTDz6LXjaH08iLfJyiZ7xCu0wcIk9kISi/xBEpuTxLLxZ69Gr0piydySULK8HMHyckC+jtuFQfvJSZ9wCWjySVzZxKQoll3f7OeS3S2zyk4WiyZ/ZVuDjTmx/qd284Z9PLdr+LbrEpE3ICW/SKGU/CKFUvKLFErJL1IoJb9IodrZrmsh8GGgz91Pr9puAD4FPF992XXufvdAxxpDvIZbVuqLtuXKSjLZ8Top52WWdtgvW88ue1TOvu9OZOW8TDaF5Nmg/dykT6ezzLJ+0XqHWXkwKwNmJbtswljWLypxzkn6nPp0/U9tMIX7du78twEX1bR/zd3nVf8GTHwRGV0GTH53v598wVgROQQN5Tn/1Wa2wswWmlm8vrOIjEqdJv83gRNprc2xCbgp+kIzm29my8xsWf3q5CLSDR0lv7tvcfd97r4fuAU4O/naBe7e4+49jS4bJCKpjpLfzGb2+/Qj5HNNRGQUaqfU9z3gfGCamW0ArgfON7N5tCoLvcCn2z1ZtPVWVjY6PWifm/TJSn2dijZcejXp0+lTnWwNwugaQvx9Z6Wmh5NYsnMVH0hi/xu0x6v+5d9XdpfKYtE2ZVm5NItl6ydm1zhbozIq9WXnin7O2e/iwQZMfne/vKb51kGcQ0RGIb3DT6RQSn6RQin5RQql5BcplJJfpFCjZgHPp5J+hwftf5/0iRb9BLgmib0tifWceUxt+6JHn69tB5iVHC/a0gryGX9Z+XB90D4n6RMtIAlxqQxa9d/I7HfWFwnXPRZvW5WVtrIxZtcjmpQyPemTyRYZzbYUy67jKx2OJeKuBTxFJKHkFymUkl+kUEp+kUIp+UUKpeQXKVSjU+z309nik9FMpTuSPn+RxLIy4AVnRoVFmP2WqBAYl/qyElVWzstKQ1m/KJatw5b9EmRltGyG3vqgpNfpzLf4CnfmN8N8PIDfj8AxR5Lu/CKFUvKLFErJL1IoJb9IoZT8IoVq9NX+2cCNQezPOzjek0nsp0kse8X52PFxdOdvs9fg68XTWGBnEts96DO1RK/OP9Ph8aJ1CyEff7TlVXbth3sbsjeyNwftg7mGuvOLFErJL1IoJb9IoZT8IoVS8osUSskvUqh2tus6DvgOMIPW9lwL3P1mMzsK+D6t5eF6gcvc/cXsWG8GPhzEfpL0i9bqW5P0+XUSOzqJbX5wYxjrJY51Irv4WakvK7Ft6nAskT0dxiKH2uSX0Spa/7F3EMdo586/F/icu58GnAN81sxOA64F7nX3k4B7q89F5BAxYPK7+yZ3f6T6eAetG+4s4BJgUfVli4BLR2qQIjL8BvWc38zmAGcCDwIz3P21vzI303paICKHiLaT38wmAXcC17j7AWtUeGvx/9oNAMxsvpktM7NlLwxpqCIynNpKfjMbRyvxv+vud1XNW8xsZhWfSbCfgbsvcPced+/JXmgTkWYNmPxmZsCtwBp3/2q/0GLgyurjK4EfD//wRGSktDOr773AJ4CVZra8arsO+BJwh5ldRWvS2GUDHWgf8Zp22TZZfxu0z0/6ZJuQZU8/srXuhnvWWTTzDWBih/3kjeOIJPaeoH3rII4/YPK7+wNAtPfX+wdxLhEZRfQOP5FCKflFCqXkFymUkl+kUEp+kUI1uoCnET/aZI9CZwXtn0/6/EtbI3q9rFRy+in1xZd7n3ylo3Nly4FOSGLZdl2Tg/ZsIVEZnY5PYhcG7b8axPF15xcplJJfpFBKfpFCKflFCqXkFymUkl+kUI2W+px4Rlo2Uy0qbX006bM6if0sie1KYqeefFp94MmHk16xbCHObBzRzEiIf6Bjkz77kliTsn0Boz0IIb+DjZbvrRNRiRvggsPr26cMYpNH3flFCqXkFymUkl+kUEp+kUIp+UUK1eir/RC/ajuIFyn/Xzbx4ZNJbG0Sy9bp29lXu0BxxzrZ7uqNTNfjQEFtCYCpwcyvsVlZ5CC684sUSskvUiglv0ihlPwihVLyixRKyS9SqAFLfWZ2HPAdWltwO7DA3W82sxuATwHPV196nbvfnR1rDPEknfFJv6j8lpUHe5LYnyWxu5LY8mXPJVHplkN58g7EE5rmJn22/q6+fRCVvrbq/HuBz7n7I2Y2GXjYzJZUsa+5+1cGcT4RGSXa2atvE7Cp+niHma0BZo30wERkZA3qOb+ZzQHOBB6smq42sxVmttDMjhzmsYnICGo7+c1sEnAncI27bwe+CZwIzKP1l8FNQb/5ZrbMzJZlW2OLSLPaSn4zG0cr8b/r7ncBuPsWd9/n7vuBW4Cz6/q6+wJ373H3nqOHa9QiMmQDJr+ZGXArsMbdv9qvfWa/L/sIsGr4hyciI6WdV/vfC3wCWGlmy6u264DLzWwerfJfL/Dpts5o9c1jkoehCUEtJ9vuamISOzeJ/TKJvXyo15Ska/4wiUVbb01L+kTrOA7mV7SdV/sfoD5l05q+iIxueoefSKGU/CKFUvKLFErJL1IoJb9IoRpfwBOvb96e1CiiGX/ZI1e2/ddbktipSWxdEhPp1OagfUPSJ/rdH8xCuLrzixRKyS9SKCW/SKGU/CKFUvKLFErJL1KoRkt9TlyKyEpz0SNUtBjoQMfLnJ7Evt3hMUV6k1hUyp6S9IlyYjD7HerOL1IoJb9IoZT8IoVS8osUSskvUiglv0ihGi31GXDY2PrYxGRWX/QI1ekjVzbz6awkdnLQ/mSH4xCBeDHOvqRPtLjnYBbw1J1fpFBKfpFCKflFCqXkFymUkl+kUAO+2m9mE4D7gcOrr/+hu19vZicAtwNHAw8Dn3D3fAmxMXDYhPrQ+J1Jv2DdP8YlfZIZDsEQgHx9vy8E7fOTPq8kMXljyTaizbaPiyaoZZPToq3qhvvV/leBC9z9nbS2477IzM4Bvgx8zd3fBrwIXDWI84pIlw2Y/N7ycvXpuOqfAxcAP6zaFwGXjsgIRWREtPWc38zGVjv09gFLgKeBl9x9b/UlG4BZIzNEERkJbSW/u+9z93nAbOBs8uXtD2Bm881smZkteyF67i4ijRvUq/3u/hJwH/BuYKqZvfaC4WxgY9Bngbv3uHvP0XUbfYtIVwyY/GZ2jJlNrT4+AvggsIbWg8BHqy+7EvjxSA1SRIZfOxN7ZgKLzGwsrQeLO9z9p2a2GrjdzG4EHgVuHfBIY8gX3ou6RTWPpGa368U4tjcOpeWVaNLPt5I+30hiDyYxGZ0mJ7HpSWxqEot+jbPq99agPfvdPtiAye/uK4Aza9rX03r+LyKHIL3DT6RQSn6RQin5RQql5BcplJJfpFDm3tzb7szseeCZ6tNpxBWLJmkcB9I4DnSojeOt7n5MOwdsNPkPOLHZMnfv6crJNQ6NQ+PQn/0ipVLyixSqm8m/oIvn7k/jOJDGcaA37Di69pxfRLpLf/aLFKoryW9mF5nZk2a2zsyu7cYYqnH0mtlKM1tuZssaPO9CM+szs1X92o4ysyVm9lT1/5FdGscNZraxuibLzeziBsZxnJndZ2arzexxM/ubqr3Ra5KMo9FrYmYTzGypmT1WjeOLVfsJZvZglTffN7PxQzqRuzf6DxhLaxmwucB44DHgtKbHUY2lF5jWhfOeR2uG8Kp+bf8MXFt9fC3w5S6N4wbg8w1fj5nAWdXHk4G1wGlNX5NkHI1eE1rbWk6qPh5Ha/b3OcAdwMer9m8BfzWU83Tjzn82sM7d13trqe/bgUu6MI6ucff7gW0HNV9CayFUaGhB1GAcjXP3Te7+SPXxDlqLxcyi4WuSjKNR3jLii+Z2I/lnAc/1+7ybi386cI+ZPWxm2fL7TZjh7puqjzcDM7o4lqvNbEX1tGDEn370Z2ZzaK0f8SBdvCYHjQMaviZNLJpb+gt+57r7WcCHgM+a2XndHhC0HvmJtyoZad8ETqS1R8Mm4KamTmxmk4A7gWvc/YCdq5u8JjXjaPya+BAWzW1XN5J/I3Bcv8/DxT9HmrtvrP7vA35Ed1cm2mJmMwGq/7Pt2UeMu2+pfvH2A7fQ0DUxs3G0Eu677n5X1dz4NakbR7euSXXuQS+a265uJP9DwEnVK5fjgY8Di5sehJm9ycwmv/YxcCGwKu81ohbTWggVurgg6mvJVvkIDVwTMzNaa0Cucfev9gs1ek2icTR9TRpbNLepVzAPejXzYlqvpD4N/EOXxjCXVqXhMeDxJscBfI/Wn497aD13u4rWVm/3Ak8BvwCO6tI4/gNYCayglXwzGxjHubT+pF8BLK/+Xdz0NUnG0eg1Ac6gtSjuCloPNP/Y73d2KbAO+AFw+FDOo3f4iRSq9Bf8RIql5BcplJJfpFBKfpFCKflFCqXkFymUkl+kUEp+kUL9H/LYXAO+QktrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i, _ = dataset[5]\n",
    "show(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "n_cpu = 1\n",
    "samples = 500\n",
    "seed = 1\n",
    "sample_interval = 5\n",
    "\n",
    "# Hyperparameters: Adam optimizer parameters (learning rate and momentum decay)\n",
    "\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "\n",
    "# Dimensions of latent space\n",
    "\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "        print(\"Using cuda.\")\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_dim // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128*self.init_size**2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, img_channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [   nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                        nn.LeakyReLU(0.2, inplace=True),\n",
    "                        nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(img_channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_dim // 2**4\n",
    "        self.adv_layer = nn.Sequential( nn.Linear(128*ds_size**2, 1),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to initialise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising and configuring metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (l1): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=8192, bias=True)\n",
      "  )\n",
      "  (conv_blocks): Sequential(\n",
      "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Upsample(scale_factor=2, mode=nearest)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Upsample(scale_factor=2, mode=nearest)\n",
      "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (9): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Dropout2d(p=0.25)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Dropout2d(p=0.25)\n",
      "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (9): Dropout2d(p=0.25)\n",
      "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (13): Dropout2d(p=0.25)\n",
      "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (adv_layer): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [16, 3, 3, 3], but got input of size [3, 32, 32] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a8301c6a193c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Measure discriminator's ability to classify real from generated samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mreal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mfake_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfake_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-17484a6da534>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mvalidity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 3, 3, 3], but got input of size [3, 32, 32] instead"
     ]
    }
   ],
   "source": [
    "gen_loss = []\n",
    "disc_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(dataset):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, epochs, i, len(dataset),\n",
    "                                                            d_loss.item(), g_loss.item()))\n",
    "\n",
    "        batches_done = epoch * len(dataset) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "    plt.plot( 'Epoch', 'Loss', data=disc_loss, marker='', color='olive', linewidth=2, label='Loss_D')\n",
    "    plt.plot( 'Epoch', 'Loss', data=gen_loss, marker='', color='blue', linewidth=2, label='Loss_G')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
